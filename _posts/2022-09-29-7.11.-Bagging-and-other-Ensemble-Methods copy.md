---
title: "[DL Book] 7-11. Bagging and other Ensemble Methods"
categories:
  - Deep Learning Book
tags:
  - Books
  - Deep Learning Book
  - Regularization
  - Statistics
  - Study Group
---

# 7.11. 앙상블 기법
### Bagging
배깅(Bagging)은 **Bootstrap Aggregating**의 줄임말로, 하나의 Task에 대해 여러 개의 모델을 독립적으로 훈련시킨 뒤, 모든 모델이 협력하여 결과 추론에 참여하도록 하는 형태를 말한다. 이 경우 하나의 테스트 샘플에 대해서 모든 모델이 잘못된 예측을 하는 일은 확률적으로 낮기 때문에 성능 향상을 기대할 수 있다.

배깅은 **Model Averaging**의 하위 개념이다. 또한 Model Averaging 전략을 사용하는 기법을 **앙상블 기법(Ensemble Method)**이라고 한다. 따라서 배깅은 수많은 앙상블 기법 중 하나인 동시에, 수많은 Model Averaging 전략 중 하나라고 이해하는 것이 옳겠다.

일반적으로 앙상블 기법에서는 여러 종류의 모델을 훈련시킨 뒤 협력하여 추론을 진행할 수 있지만, **배깅에서는 같은 종류의 모델만을 사용하는 것이 원칙이다.** 다음은 일반적인 배깅의 진행 절차이다.

1. 크기가 $k$인 학습 데이터셋에서 $k$번 복원추출을 시행한다.
2. 그러면 원본과 크기가 같지만 일부 데이터가 누락되고, 일부는 중복되는 데이터셋이 생긴다.
    - **이 부분이 배깅의 핵심이다. 여러 모델 사이의 차이가 나타나는 원인이기 때문이다.**
3. 이것을 $n$번 시행해 $n$개의 다른 데이터셋을 만든다.
4. 같은 종류의 머신러닝 모델을 $n$개 생성하고, 각기 다른 데이터셋으로 훈련시킨다. 

### 심층신경망에서의 Bagging

배깅의 특징은 비슷한 조건 아래에서 훈련시킨 여러 모델 사이의 차이를 이용하는 것이다. 이를 위해 배깅을 시행할 때에는 약간씩 다른 데이터셋을 생성해서 각 모델을 훈련시킨다. 하지만 심층신경망의 경우 같은 훈련 데이터로 학습시키더라도, 매번 다른 해로 수렴할 만큼 많은 해의 가능성이 있다. 따라서 심층신경망에 대해 배깅을 진행할 때는 추출을 통한 데이터셋 생성은 필요하지 않은 경우가 많다. 자연스럽게 발생하거나 하이퍼파라미터를 조절함으로써 발생하는 최적화 해의 차이로도 충분히 배깅을 진행할 수 있다.

### 불문율

앙상블 기법은 상당히 강력한 일반화 성능 향상 도구이다. 머신러닝 모델의 성능을 경쟁하는 대회에서 앙상블 기법은 **거의 무조건** 등장한다. 하지만 그 어떤 머신러닝 모델이라도 앙상블 기법을 통해 일반화 성능을 끌어올릴 수 있기에, 논문 등에 모델의 성능을 기록할 때에는 앙상블 기법을 사용하지 않는 것이 불문율이다.

> :bulb: **Yellokat 의 말 :**
> 
> 책에서 언급된 Netflix Grand Prize (Koren, 2009)는 추천시스템에 관심이 있는 저로써는 수없이 곱씹어 읽고 구현해 본 근본있는 논문입니다. 처음 접한 지 3년 가량 되었는데도 앙상블 기법을 사용했다고 논문에 직접 언급이 없어 전혀 모르고 있었네요. 검색해 보니 무려 **107개**의 모델을 추론에 사용했다고 합니다.