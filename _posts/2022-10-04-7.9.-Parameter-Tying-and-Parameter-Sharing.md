---
title: "[DL Book] 7-9. Parameter Tying and Parameter Sharing"
categories:
  - Deep Learning Book
tags:
  - Books
  - Deep Learning Book
  - Regularization
  - Statistics
  - Study Group
---

# 7.9. Parameter Tying and Parameter Sharing

두 개 이상의 모델의 가중치를 비슷해지도록 제한하는 것을 **Parameter Tying**이라고 하며, 두 개 이상의 모델의 가중치가 완전히 일치하고 공유되도록 하는 것을 **Parameter Sharing**이라고 한다. (Parameter Sharing 은 **7.7. Multitask Learning**에서도 언급되었다.)

### Parameter Tying

예를 들면 L2 정칙화와 비슷한 방식으로 두 모델의 가중치가 비슷해지도록 강제할 수 있다. 더 구체적으로는,

$$
\Omega(\bm w^{(A)}, \bm w^{(B)}) = ||\bm w^{(A)}-\bm w^{(B)}||^2_2
$$

와 같은 형태로 정칙화 항을 구성한다면, 두 가중치 사이의 유클리디안 거리가 줄어드는 방향으로 최적화가 진행된다. 물론 L2가 아닌 다른 방식으로도 정칙화 항을 구성할 수 있다.

### Parameter Sharing

위에서 설명한 Parameter Sharing보다 더 강한 규제는 두 가중치가 완전히 일치할 것을 강제하는 것이다. 또한, 엔지니어링 측면에서 Parameter Sharing 큰 장점 중 하나는 **메모리 소모가 줄어든다는 것**이다.

### 합성곱 신경망

Parameter Sharing을 이용한 가장 대중화된 모델은 역시 이미지 인식에서 자주 활용되는 합성곱 신경망일 것이다. 이미지 데이터의 경우 상하좌우로 $n$픽셀씩 이동시켜도 정답 레이블에 영향을 미치지 않는 경우가 대다수이다. 따라서 합성곱 신경망에서는 입력 이미지의 여러 위치에서 같은 가중치 필터를 사용해 해당 특징(feature)의 유무를 판별한다. 이러한 구조로 인해 합성곱 신경망은 고양이가 사진 한가운데에 있든, 오른쪽 귀퉁이에 숨어있든 관계없이 고양이라는 특징을 찾아낼 수 있는 것이다.

또한, Parameter Sharing을 통해 메모리 공간을 크게 절약함으로 인해 합성곱 신경망은 크기를 비교적 수월하게 늘리는 것이 가능하며, 네트워크가 커지고 깊어져도 그 규모에 비해 추가 학습 데이터가 비교적 적게 필요하다.