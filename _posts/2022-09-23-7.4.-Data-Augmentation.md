---
title: "[DL Book] 7-4. Data Augmentation"
categories:
  - Deep Learning Book
tags:
  - Books
  - Deep Learning Book
  - Regularization
  - Statistics
  - Study Group
---

머신러닝 모델의 일반화 성능을 올리는 가장 좋은 방법 중 하나는 **더 많은 데이터로 학습시키는 것**이다. 하지만 현실에서 우리가 사용할 수 있는 데이터는 제한되어 있으므로, 현실에 있을 법한 **가짜 데이터**를 만들어서 추가적으로 학습에 사용하는 것이 데이터 증강의 핵심이다.

데이터 증강은 분류 문제에 특히나 효율적이며, 이미지 데이터를 사용한다면 다양하고 강력한 증강 전략을 사용할 수 있다. 이에 따라 데이터 증강은 **물체 인식(Object Detection)** 작업에 특출난 성능을 보인다.

### 이미지 데이터 증강 전략 예시

- 상/하/좌/우로 $n$ 픽셀 옮기기
- 상하 혹은 좌우 반전시키기
- 확대 혹은 축소하기
- 회전시키기
- 노이즈 주입

### 주의점

- **분류 문제에는 적합하지만, 다른 문제에는 적합하지 않을 수 있다.**
이것은 분류 문제의 경우 입력 데이터에 다채로운 변화(Transformation)를 주어도, 이산 데이터인 출력값은 비교적 덜 민감하기 때문이다. 예를 들면 분포 추정 문제의 경우 입력의 변화에 따라 출력값이 민감하게 변할 수 있으며, 데이터를 생성하는 분포를 모르는 상태에서 가짜 데이터를 만드는 것도 한계가 있다.
- **데이터 성격에 맞는 증강 전략을 사용해야 한다.**
예를 들면, 숫자 이미지를 분류하는 뉴럴 네트워크를 학습한다고 하자. 이 경우 “6”이 적힌 사진의 정답 레이블은 6이다. 이 이미지를 180도 회전하여 추가 데이터로 사용한다면 “9”라고 적힌 사진을 6이라고 가르치는 꼴이 된다! 이처럼 일부 데이터 증강 전략은, 그 결과로 생성되는 데이터가 현실에 있을 법한지를 꼭 염두에 두고 사용해야 한다.
- **데이터 증강을 진행한 실험의 결과 비교에 유의해야 한다.**
    
    예를 들어 머신러닝 모델 A와 B가 있을 때, 증강된 데이터로 학습된 모델 A가 일반 데이터로 학습된 모델 B보다 우수한 성능을 보인다고 해도, 둘의 성능은 비교할 수 없거나 B가 오히려 우수한 모델일 수 있다. 두 모델을 동일선상에서 비교하기 위해서는 **입력 데이터가 주어지는 조건이 동일해야 한다.**
    

### 연속적인 수치 데이터 증강하기

이미지가 아닌 데이터여도 증강이 가능하다. 연속적인 수치를 나타내는 데이터는 Random Noise를 주입하는 것으로 가짜 데이터를 생성할 수 있다. 예를 들면 머신러닝 모델의 입력값에 노이즈를 더하거나, 심층신경망의 각 층에서 노이즈를 더해 주는 방식이다. 특히, 심층신경망의 각 층에서 노이즈와의 곱셈을 진행하는 방식은 7.12에서 다룰 Dropout과도 밀접한 관련이 있다.

단, 심층신경망은 머신러닝 모델에 비해 노이즈 주입에 비교적 취약한 면이 있다고 한다.


> :bulb: **Yellokat의 말:**
> 
> Data Augmentation이 왜 Regularization Strategy인지 잘 이해가 안 된다면, 큰 의미에서 Regularization의 정의를 다시 읽어보면 좋습니다. 이 챕터의 나머지 부분에서 등장하는 기법들도 여러 분야를 아우르며 어수선하다고 느낄 수 있지만, 결국 같은 정의를 통해 한 곳에 묶어집니다.
> 
> **“정칙화(Regularization)는 훈련 오차와는 무관하지만 일반화 오차를 줄이는 모든 전략을 말한다.”**
> **- Deep Learning, pg. 221.**
> 