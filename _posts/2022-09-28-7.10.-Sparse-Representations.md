---
title: "[DL Book] 7-10. Sparse Representations"
categories:
  - Deep Learning Book
tags:
  - Books
  - Deep Learning Book
  - Regularization
  - Statistics
  - Study Group
---

# 7.10. Sparse Representations
지금까지 알아본 L2과 L1 정칙화에서는 정칙화 항을 $\Omega(\theta)$, 즉 모델의 가중치에 대한 함수로 사용했다. 또한 L1 정칙화는 가중치의 많은 값들을 0으로 만드는 특징(Sparsity)이 있다고 했다. 같은 맥락에서 정칙화 항의 함수를 $\Omega(\boldsymbol h)$, 즉 표현(representation)에 대한 함수로 사용하면, **L1 정칙화가 표현의 대다수 값을 0으로 만든다**. 다음의 예시를 보자.

![Untitled](/assets/images/dlbook/7/7.png)

위 수식은 $\Omega(\theta)$로 정의된 L1 정칙화를 통해 모델의 가중치의 대다수 값이 0이 된 모습을 보여 준다. 반면, 아래 수식은 $\Omega(\boldsymbol h)$로 정의된 L1 정칙화를 통해 표현 $\boldsymbol h=f(x)$의 대다수 값이 0 된 모습을 보여 준다.

중요한 것은 정칙화 항을 $\boldsymbol h$에 대한 함수로 사용하는 것이다. L1 정칙화를 사용하는 것은 Sparsity를 유도하기 위한 많은 방법 중 하나일 뿐이다. L1 노름을 사용한 정칙화 항 이외에도 Student-t 분포, KL-Divergence 등을 이용한 정칙화 항 등이 알려져 있다.

### 왜 희소성(Sparsity)에 집착하는가?

0이 아닌 값만을 저장한다고 생각하면 희소한 데이터는 저장공간 소비가 매우 효율적으로 변한다. 

> :bulb: **yellokat의 말 :**
> 
> 책에는 Sparse Parametrization/Representational Sparsity의 기법자체만 소개되어 있고, 그게 왜 의미있는지는 나오지 않네요.
