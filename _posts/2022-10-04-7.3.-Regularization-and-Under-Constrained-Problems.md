---
title: "[DL Book] 7-3. Regularization and Under-Constrained Problems"
categories:
  - Deep Learning Book
tags:
  - Books
  - Deep Learning Book
  - Regularization
  - Statistics
  - Study Group
---

### 잘 정의되지 않은 선형회귀

잘 정의되지 않은 몇몇 기계학습 문제들은 정칙화를 통해 잘 정의된 문제로 바꿀 수 있다. 예를 들면 오차함수가 Sum Squared Errors로 설정된 선형회귀의 해를 살펴보자. (이 문제는 7.1.1에서도 다루었다.)

$$
\bm{w=(X^{\text{T}}X)^{-1}X^{\text{T}}y} \hspace{1.5cm}\text{Unregularized Linear Regression with SSE}
$$

최적해는 $X^{\text{T}}X$의 역행렬을 이용해서 구하는 경우가 많다. 하지만 이는 $X^{\text{T}}X$가 역행렬을 가지지 않는 특이행렬(Singular Matrix)일 경우에는 성립하지 않는다. 예를 들면 아래와 같은 경우 $X^{\text{T}}X$의 역행렬이 존재하지 않을 수 있다.

- 특정 변수(feature)가 모든 점에서 완전히 같을 때
- 변수의 차원 수보다 입력된 데이터의 수가 작을 때

이러한 경우 정칙화를 이용하여 $X^{\text{T}}X+\color{red}\alpha \bm I$의 역행렬을 대신 찾는 형태로 문제를 풀 수 있다. 정칙화 항의 대각원소가 모두 양수이기에, 정칙화된 이 행렬은 반드시 역행렬을 가진다.

$$
\bm{w=(X^{\text{T}}X+\color{red}\alpha I\color{black})^{-1}X^{\text{T}}y} \hspace{1.5cm}\text{L2 Regularized Linear Regression with SSE}
$$

### 잘 정의되지 않은 로지스틱 회귀

로지스틱 회귀의 경우, 두 클래스가 완전히 선형적으로 분리가 가능할 경우 오류가 발생한다는 것은 널리 알려진 사실이다. 이는 잘 정의되지 않은 문제로 볼 수 있다.

만약 데이터를 완벽하게 분리하는 벡터 $\bm w$가 존재한다면, 벡터 $2\bm w$역시 데이터를 완벽하게 분리할 것이다. 따라서 Iterative Method 를 통해 경계를 찾고자 한다면 알고리즘이 수렴하지 않는다. 알고리즘은 가중치 벡터 $\bm w$의 크기를 계속해서 키워 가면서 탐색을 계속할 것이다.

그럼 여기서 정칙화의 효과를 기억해 보자. 정칙화는 매 단계에 가중치를 갱신하기 전에, 가중치의 절댓값을 축소시킨다. 따라서 같은 방향을 가리키지만 크기만 더 큰 벡터를 찾아 알고리즘이 무한히 탐색하는 현상을 막을 수 있다.

물론 현실에서는 가중치 값이 무한히 커진다면 오버플로우가 발생하므로 알고리즘은 언젠가 종료되기 마련이다. 

### 머신러닝 이외의 영역에서의 정칙화

정칙화는 머신러닝의 영역 밖에서도 활약한다. 대표적인 예로 선형대수학에서 종종 사용되는 무어-펜로즈 유사역행렬을 떠올려보자.

$$
\bm{X^{+}=\lim_{\alpha\searrow0}(X^{\text{T}}X+\alpha I)^{-1}X^{\text{T}}} \hspace{1cm} \text{Moore-Penrose Psuedoinverse of } \bm X
$$

그러면 유사역행렬을 구하는 수식이, 바로 위에서 살펴본 Sum Squared Errors를 오차함수로 사용하고 L2 정칙화를 적용한 선형회귀의 수식과 매우 유사함을 볼 수 있다! 즉, 특이행렬의 역행렬을 구하라는 요구는 잘 정의되지 않은 문제이다. 특이행렬에 대해 역행렬 대신 무어-펜로즈 유사역행렬을 구하는 것은, L2 정칙화를 이용해서 잘 정의되지 않은 문제에 대한 최선의 해를 내놓는 행동인 것이다.