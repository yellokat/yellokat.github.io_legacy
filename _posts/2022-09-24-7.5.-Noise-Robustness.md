---
title: "[DL Book] 7-5. Noise Robustness"
categories:
  - Deep Learning Book
tags:
  - Books
  - Deep Learning Book
  - Regularization
  - Statistics
  - Study Group
---

# 7.5. Noise Robustness
1절에서 설명했듯이, 일부 모델의 경우 입력값에 매우 작은 노이즈를 주입하는 것은 모델의 가중치에 페널티를 부가하는 것과 같다. 하지만 심층신경망에서 입력값이 아닌 은닉층에서 노이즈를 주입하는 것은 단순한 가중치 감쇠보다 더 큰 의미가 있다. 이 기법에 대한 연구는 매우 방대해 독자적인 분야가 있으며, 잘 알려진 Dropout이 이 기법의 특수한 경우이다.

### 가중치에 노이즈 더하기

베이즈 추정을 이용해 가중치를 확률적으로 추론하는 접근이다. 기존 가중치 갱신은 단순히 수치의 덧, 뺄셈으로 이루어지는 반면, 가중치에 불확실성이 존재하며 어떤 확률분포를 따를 것을 예상하고, 그 분포를 모델링하는 방식이라고 볼 수 있다. 이 불확실성을 적용하는 가장 쉬운 방법 중 하나가 바로 노이즈 주입인 것이다.

### 출력값에 노이즈 더하기

데이터셋의 레이블에 오류가 있을 수도 있다. 이 경우, $\log p(y\|\boldsymbol x)$를 최대화하는 방식으로 모델을 학습하는 것은 위험할 수 있다. 이 경우 리스크를 피하기 위해 레이블에 노이즈가 있음을 가정할 수 있다.

- 레이블 $y$는 $1-\epsilon$의 확률로 올바르게 표기되었다.
- 레이블 $y$는 $\epsilon$의 확률로 잘못 표기되었다.

이렇게 디자인된 정답 레이블의 불확실성은 수학적으로 표현하기도 수월하다. 예를 들면 다중 분류 문제에서 정답 레이블에 이러한 불확실성을 반영하기 전과 후의 결과는 다음과 같다. 예시에서는 $0.03$을 사용하였다.


|  | 개 | 고양이 | 햄스터 | 토끼 |
| --- | --- | --- | --- | --- |
| 기존 | 0 | 1 | 0 | 0 |
| Label Smoothing 적용 | 0.01 | 0.97 | 0.01 | 0.01 |


즉 정답 레이블이 나타내는 확률분포를 조금 더 부드러운 모양으로 바꾸어, 오답 레이블에도 낮은 수치이지만 약간의 기회를 주는 것이다. 정답 레이블을 부드러운 모양의 분포로 바꾸는 기법이기에 이것을 **Label Smoothing**이라고 부른다.